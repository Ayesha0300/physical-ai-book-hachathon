# VLA Module Glossary

## A

**Action Component**: The component of a VLA system that executes physical behaviors based on interpreted commands, managing high-level task planning, low-level motor control, safety monitoring, and feedback integration.

## C

**Cognitive Planning**: The process by which LLMs decompose complex goals into sequences of executable actions, handling uncertainty and re-planning when necessary.

## L

**Large Language Model (LLM)**: A type of artificial intelligence model trained on vast amounts of text data that can understand and generate human-like language, used in VLA systems for planning and interpretation.

## P

**Perception System**: A vision-based system that recognizes objects and understands scenes to ground language references in physical reality.

## R

**ROS 2 (Robot Operating System 2)**: A flexible framework for writing robot software that provides hardware abstraction, device drivers, libraries, visualizers, message-passing, and package management.

## S

**Symbol Grounding**: The challenge of connecting abstract symbols (words) to concrete physical entities in the environment.

## V

**Vision-Language-Action (VLA)**: A paradigm that integrates visual perception, language understanding, and physical action execution to enable more intuitive human-robot interaction.

**Voice-to-Action Pipeline**: The complete process from audio capture through speech recognition, intent parsing, and action mapping to robot execution.
